---
title: "Random Forest - Classification Project"
author: "Piotr Wieczorek"
date: "2023-07-08"
output: 
  html_document: 
    theme: lumen
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE)
```

## This project aims to classify pumpkin seeds

### Libraries

```{r}
library(tidyverse)
library(randomForest)
library(janitor)
library(readxl)
library(caret)
library(glue)
library(kableExtra)
```


```{r}
df <- read_excel("Pumpkin_Seeds_Dataset.xlsx")
```


```{r}
df %>%
  head() %>%
  kable(align = "c") %>%
  kable_styling(position = "center") %>%
  scroll_box(width = "100%")
```

```{r}
df <- df %>%
  mutate_if(is.character,as.factor)
```

```{r}
sapply(df, function(x) sum(is.na(x)))
```
```{r}
set.seed(567)
split_index <- createDataPartition(df$Class,p=0.7, list = FALSE)
train_df <- df[split_index,]
test_df <- df[-split_index,]
print(nrow(train_df))
print(nrow(test_df))
```
```{r}
print(glue('ncol = {ncol(train_df)}'))
print(glue('ncol^0.5 = {round(ncol(train_df)^0.5,2)}'))
```
### Usually it's recommended to use square root of ncol as the number of randomly assigned variables to each tree. In this example we would use mtry = 3 or mtry = 4. The OOB error rate is smaller for mtry = 3. 


```{r}
set.seed(831)
rf_mod <- randomForest(Class ~., data=train_df, ntree = 1000, mtry = 3,
                       importance = TRUE)
```

```{r}
rf_mod
```
### ~500 decision trees would be sufficient, since the OOB error rate is stable after this threshold.

```{r}
plot(rf_mod)
```


### Importance plot - Mean decrease in accuracy

```{r,fig.width=10}
varImpPlot(rf_mod,type=1)
```

### Using the model on the test_df dataframe

```{r}
head(predict(rf_mod,test_df))
```
```{r}
test_df$prediction <- predict(rf_mod,test_df)
```

```{r}
test_df$correct_prediction <- ifelse(test_df$Class == test_df$prediction, "Yes","No")
```

```{r}
test_df %>%
  head() %>%
  kable(align = "c") %>%
  kable_styling(position = "center") %>%
  scroll_box(width = "100%")
```
```{r}
table(test_df$correct_prediction)
```
### Accuracy score

```{r}
1 - (93/(93+657))
```
### Confusion matrix - the model works well, because the accuracy score (0.88) is much greater than No Information Rate (0.52). No information rate indicates that if we were guessing Çerçevelik all the time, we would guess 52% of this class correctly.

```{r}
confusionMatrix(test_df$prediction, reference = test_df$Class)
```

### Here's where No Information Rate : 0.52 results from

```{r}
janitor::tabyl(test_df$Class)
```



