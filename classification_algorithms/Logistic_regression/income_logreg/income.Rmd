---
title: "Income Classification - Logistic Regression Example"
author: "Piotr Wieczorek"
date: "2023-05-20"
output: 
  html_document: 
    theme: lumen
    toc: yes
    toc_float: yes
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE,warning = FALSE, message = FALSE)
```

## Goal of this project

### The goal of this project is to use logistic regression to predict the income group (either >50K or <=50K) based on the following variables:
<h4>1. age</h4>
<h4>2. work class</h4>
<h4>3. education</h4>
<h4>4. marital status</h4>
<h4>5. occupation</h4>
<h4>6. race</h4>
<h4>7. sex</h4>
<h4>8. hours per week (of work)</h4>
<h4>9. native country</h4>

## Libraries and reading the data

```{r}
library(tidyr)
library(tidyverse)
library(ROCit)
library(caret)
library(corrplot)
library(fastDummies)
library(performance)
library(psych)
library(gridExtra)
library(rcompanion)
library(coefplot)
library(kableExtra)
library(formatR)
library(pROC)
```


```{r}
rm(list = ls())
```


```{r}
df <- read_csv("C:/Users/piotr/Desktop/R_files/adult.csv")
```


## Exploring the data set


```{r}
colnames(df)
```
### Notice that several "?" signs occur in the data - they will be replaced with NaN values

```{r}
str(df)
```
```{r}
length(df[df=="?"])
```

```{r}
df[df=="?"] <- NA
```

```{r}
sum(is.na(df))
```

### It occurs that each column has the same number of NA (that originally were the "?" signs)

```{r}
sapply(df, function(x) sum(is.na(df)))
```
### Before omitting NA values, let's choose only these columns that will be used in the model

```{r}
df <- df %>%
  select(c("age","workclass","education","marital.status","occupation","race","sex","hours.per.week","native.country","income"))
```

### NaN values couldn't be replaced in this example e.g with mean, because of the specification of remained columns

```{r}
df <- na.omit(df)
```

### Let's describe numeric variables

```{r}
df %>%
  select_if(is.numeric) %>%
  describe() %>%
  kable(align = "c") %>%
  kable_styling(position="center") %>%
  scroll_box(width = "100%",height = '200px')
```

### Change character variables to factors

```{r}
df <- df%>%
  mutate_if(is.character,as.factor)
```

```{r}
df %>%
  select_if(is.factor) %>%
  colnames()
```

### In this part, some categories of factor variables were aggregated in other categories in order to not only simplify the model, but also to get rid of near zero variance variables

```{r}
count(df,education,sort = TRUE)
```

```{r}
df <- df %>%
  mutate(education = fct_collapse(education, 
                                  "Higher_education" = c("Bachelors","Masters"),
                                  "High_school_graduate" = "HS-grad",
                                  other_level = "Other_education")) 
```


```{r}
count(df,workclass,sort = TRUE)
```
```{r}
df <- df %>%
  mutate(workclass = fct_collapse(workclass, 
                                  "Gov_employed" = c("Local-gov","State-gov","Federal-gov"),
                                  "Self_employed" = c("Self-emp-not-inc","Self-emp-inc")))
```

### Note that without-pay category occurs only 14 times (it would be a near-zero-variance dummy variable). It was decided to delete these 14 rows from the dataset, as there is no real logical way to combine it with other categories

```{r}
df <- subset(df, !(workclass %in% "Without-pay"))
df$workclass <- droplevels(df$workclass)
```


```{r}
count(df,marital.status,sort = TRUE)
```

```{r}
df<- df %>%
  mutate(marital.status = fct_collapse(marital.status,
                                       "Married" = c("Married-civ-spouse","Married-spouse-absent","Married-AF-spouse"),
                                      other_level = "Not_married"))
```



```{r}
count(df,occupation,sort = TRUE)
```

### Armed-Forces (n=9) and Priv-house serv (n=143) were deleted because there was no logical way to combine these categories with another categories and otherwise they would be near-zero-variance dummy variables
```{r}
df <- subset(df,!(occupation %in% c("Armed-Forces","Priv-house-serv")))
df$occupation <- droplevels(df$occupation)
```


```{r}
count(df,race,sort = TRUE)
```
```{r}
df <- df %>%
  mutate(race = fct_collapse(race,
                             "Other_race" = c("Other","Amer-Indian-Eskimo","Asian-Pac-Islander")))
```

```{r}
count(df,sex,sort = TRUE)
```

```{r}
count(df,native.country,sort = TRUE)
```

```{r}
df <- df %>%
  mutate(native.country = fct_collapse(native.country,
                                       "USA" = c("United-States"),
                                       "Europe" = c("Germany","England","Poland","Italy","Portugal","Greece","France",
                                                    "Ireland","Yugoslavia","Hungary","Scotland","Holand-Netherlands"),
                                       other_level = "Other"))%>%
  rename("Native_region" = "native.country")
```



```{r}
count(df,income,sort = TRUE)
```

## Additional Data exploration

### It's clear that mean, median and mode of age is higher for >50k income
```{r}
df %>%
  ggplot(aes(x=age,fill=income)) +
  geom_density(alpha=0.5)+
  scale_fill_manual(values=c("#76BBC3","#84BC8E")) +  
  theme_bw()+
  ggtitle("Age Density by Income")
```

### Based on the plots below, one could conclude that:
<h4>1. There is probably no significant correlation between race and income</h4>
<h4>2. People who are married should be expected to have a higher income</h4> 
<h4>3. Native region seems to be somewhat informative, as people from outside Europe and USA in general have a lower income</h4>
<h4>4. Higher education seems to be highly related to higher income</h4>
<h4>5. Work class seems to be irrelevant as the differences between categories are low</h4>
<h4>6. Occupation plays an important role especially considering executive categories</h4> 
<h4>7. Sex is also an important factor as men tend to have a higher income than women</h4>

```{r,fig.width=12,fig.height=14}
grid.arrange(
  count(df,race,income) %>%
  group_by(race) %>%
  mutate(percent = n/sum(n)) %>%
  mutate(percent = round(percent,2)) %>%
  ggplot(aes(x=race,y=percent,fill=income)) +
  geom_bar(stat="identity") + 
  scale_fill_manual(values=c("#76BBC3","#84BC8E")) + 
  theme_bw()+
  xlab("Race")+
  ylab("%")+
  ggtitle("Race Percentage by Income")+
  theme_bw(),
  
  count(df,marital.status,income) %>%
  group_by(marital.status) %>%
  mutate(percent = n/sum(n)) %>%
  mutate(percent = round(percent,2)) %>%
  ggplot(aes(x=marital.status,y=percent,fill=income)) +
  geom_bar(stat="identity") + 
  scale_fill_manual(values=c("#76BBC3","#84BC8E")) + 
  theme_bw()+
  xlab("Marital status")+
  ylab("%")+
  ggtitle("Marital Status Percentage by Income")+
  theme_bw(),
  
  count(df,Native_region,income) %>%
  group_by(Native_region) %>%
  mutate(percent = n/sum(n)) %>%
  mutate(percent = round(percent,2)) %>%
  ggplot(aes(x=Native_region,y=percent,fill=income)) +
  geom_bar(stat="identity")+
  scale_fill_manual(values=c("#76BBC3","#84BC8E")) + 
  xlab("Native region")+
  ylab("%")+
  ggtitle("Native Region Percentage by Income") + 
  theme_bw(),
  
  count(df,education,income) %>%
  group_by(education) %>%
  mutate(percent = n/sum(n)) %>%
  mutate(percent = round(percent,2)) %>%
  ggplot(aes(x=education,y=percent,fill=income)) +
  geom_bar(stat="identity")+
  scale_fill_manual(values=c("#76BBC3","#84BC8E")) + 
  xlab("Education")+
  ylab("%")+
  ggtitle("Education Percentage by Income") + 
  theme_bw(),
  
  count(df,workclass,income) %>%
  group_by(workclass) %>%
  mutate(percent = n/sum(n)) %>%
  mutate(percent = round(percent,2)) %>%
  ggplot(aes(x=workclass,y=percent,fill=income)) +
  geom_bar(stat="identity")+
  scale_fill_manual(values=c("#76BBC3","#84BC8E")) + 
  xlab("Workclass")+
  ylab("%")+
  ggtitle("Workclass Percentage by Income")+
  theme_bw(),
  
  count(df,occupation,income) %>%
  group_by(occupation) %>%
  mutate(percent = n/sum(n)) %>%
  mutate(percent = round(percent,2)) %>%
  ggplot(aes(y=occupation,x=percent,fill=income)) +
  geom_bar(stat="identity")+
  scale_fill_manual(values=c("#76BBC3","#84BC8E")) + 
  ylab("Occupation")+
  xlab("%")+
  ggtitle("Occupation Percentage by Income")+
  theme_bw(),
  
  count(df,sex,income) %>%
  group_by(sex) %>%
  mutate(percent = n/sum(n)) %>%
  mutate(percent = round(percent,2)) %>%
  ggplot(aes(y=sex,x=percent,fill=income)) +
  geom_bar(stat="identity")+
  scale_fill_manual(values=c("#76BBC3","#84BC8E")) +
  ylab("Sex")+
  xlab("%")+
  ggtitle("Sex Percentage by Income") + 
  theme_bw(),
  
  count(df,income) %>%
  ggplot(aes(x=income,y=n,fill=income)) + 
  geom_bar(stat="identity")+
  scale_fill_manual(values=c("#76BBC3","#84BC8E")) + 
  xlab("Income")+
  ylab("Frequency")+ 
  ggtitle("Frequency of Income") + 
  theme_bw(),

  
  ncol = 2,nrow=4
)
```






### Let's explore the variable's dependency based on the Chi-squared test and the Cramer V score


```{r}

```

```{r}
df_chi <- data.frame()
for (i in c("workclass","education","marital.status","occupation","race","sex","Native_region")){
  chi_test = chisq.test(df[[i]],df$income)
  chi_stat = chi_test$statistic
  cramer = cramerV(df[[i]],df$income)
  variable_name = i
  output = c(chi_stat,cramer,variable_name)
  df_chi = rbind(df_chi,output)
}

colnames(df_chi) <- c("chisq_stat","cramer's v","var_name")

df_chi <- df_chi %>%
  select(3,1,2) %>%
  mutate(chisq_stat = as.numeric(chisq_stat),
         `cramer's v` = as.numeric(`cramer's v`)) %>%
  mutate(chisq_stat = round(chisq_stat,2),
         `cramer's v` = round(`cramer's v`,2)) %>%
  arrange(desc(`cramer's v`)) 

df_chi %>%
  kable(align = "c") %>%
  kable_styling(position="center") 

```


## Modelling part - first model

### Data partition
```{r}
set.seed(355)
partition <- createDataPartition(y = df$income, p = 0.7, list = FALSE)
train_set <- df[partition,]
test_set <- df[-partition,]

print(nrow(train_set))
print(nrow(test_set))
print(nrow(train_set)/nrow(df))
print(nrow(test_set)/nrow(df))
```


### Let's try the first model with all variables. It's clear that it's not the best approach and as expected, some variables contribute very little to the model

```{r}
first_model <- glm(formula = income~.,
                   data = train_set,
                   family="binomial")
summary(first_model)
```
```{r}
prediction <- predict(object = first_model, test_set,type="response")
```
```{r}
head(prediction)
```
```{r}
prediction_group <- ifelse(prediction>=0.5,">50K","<=50K")
```
```{r}
head(prediction_group)
```
### Confusion matrix for the first model - 81.93% accuracy score with all variables included



```{r}
confusionMatrix(data = as.factor(prediction_group), reference = test_set$income,positive = ">50K")
```

### AUC Score

```{r}
sim_roc1 <- roc(response = test_set$income,
               predictor = prediction,
               levels = c('>50K','<=50K'))
```

```{r}
auc(sim_roc1)
```


## Second model

### In this model race and work class were excluded
```{r}
second_model <- glm(formula = income~. -race -workclass,
                   data = train_set,
                   family="binomial")
summary(second_model)
```


```{r}
prediction_second <- predict(object = second_model, test_set,type="response")
```

```{r}
head(prediction_second)
```


```{r}
prediction_second_group <- ifelse(prediction_second>=0.5,">50K","<=50K")
```
```{r}
head(prediction_second_group)
```


### Confustion matrix for the second model with 81.81% accuracy score, but with fewer variables which overall makes the model less complicated
```{r}
confusionMatrix(data = as.factor(prediction_second_group), reference = test_set$income,positive = ">50K")
```

### AUC Score

```{r}
sim_roc2 <- roc(response = test_set$income,
               predictor = prediction_second,
               levels = c('>50K','<=50K'))
```

```{r}
auc(sim_roc2)
```



## Third model

### This model excludes race, workclass and native region. Note that all of these 3 variables were the least related to the dependent variable (income) based on the Cramer's V result. This time all of the variables are statistically significant.

```{r}
third_model <- glm(formula = income~. -race -workclass -Native_region,
                   data = train_set,
                   family="binomial")
summary(third_model)
```

### Coefficient plot for the third model

```{r}
coefplot(third_model) + theme_bw()
```

### From the coefficient plot and from the model's result it is clear that:
<h4>1. Male workers are more likely to have a higher income than female workers</h4>
<h4>2. Tranpost-moving workers are less likely to have a higher income compared to Adm-clerical workers</h4>
<h4>3. Tech support workers are more likely to have a higher income compared to Adm-clerical workers</h4>
<h4>4. Sales workers are more likely to have a higher income compared to Adm-clerical workers</h4>
<h4>5. Protective-Serv workers are more likely to have a higher income compared to Adm-clerical workers</h4>
<h4>6. Protective-Serv workers are more likely to have a higher income compared to Adm-clerical workers</h4>
<h4>7. Prof-specialty workers are more likely to have a higher income compared to Adm-clerical workers</h4>
<h4>8. Other service workers are less likely to have a higher income compared to Adm-clerical workers</h4>
<h4>9. Machine-op-inspct workers are less likely to have a higher income compared to Adm-clerical workers</h4>
<h4>10. Handlers-cleaners workers are less likely to have a higher income compared to Adm-clerical workers</h4>
<h4>11. Farming-fishing workers are less likely to have a higher income compared to Adm-clerical workers</h4>
<h4>12. Exec-managerial workers are more likely to have a higher income compared to Adm-clerical workers</h4>
<h4>13. Craft-repair workers are less likely to have a higher income compared to Adm-clerical workers</h4>
<h4>14. Not married workers are less likely to have a higher income compared to married workers</h4>
<h4>15. Workers with "Other education" label are less likely to have a higher income compared to workers with higher education</h4>
<h4>16. Workers with "high school" education are less likely to have a higher income compared to workers with higher education</h4>
<h4>17. More hours per week results in better chance for getting a higher income</h4>
<h4>18. Increase in age results in better chance for getting a higher income</h4>

### To inspect the exact result of these measures, it would be helpful to calculate the exp() of each coefficient.

```{r}
exp(third_model$coefficients)
```
### Interpretation:
<h4>1. One unit increase in age results in **2.93% higher odds** for getting the above 50k income label</h4>
<h4>2. One unit increase in hours.per.week results in **3.33% higher odds** for getting the above 50k income label</h4>
<h4>3. Having a high school graduate category results in **58.92% lower odds (compared to Higher education)** for getting the above 50k income label</h4>
<h4>4. Having Other_education category results in **51.26% lower odds (compared to Higher education)** for getting the above 50k income label</h4>
<h4>5. Not being married results in **88.74% lower odds (compared to being married)** for getting the above 50k income label</h4>
<h4>6. Working in occupation craft field results in 23.4% lower odds (compared to Adm-clerical) for getting the above 50k income label</h4>
<h4>7. Working in Exec-managerial field results in **138.63% higher odds (compared to Adm-clerical)** for getting the above 50k income label</h4>
<h4>8. Working in Farming-fishing field results in **77.74% lower odds (compared to Adm-clerical)** for getting the above 50k income label</h4>
<h4>9. Working in Handlers-cleaners field results in **69.84% lower odds (compared to Adm-clerical)** for getting the above 50k income label</h4>
<h4>10. Working in Machine-op-inspct field results in having **50.88% lower odds (compared to Adm-clerical)** for getting the above 50k income label</h4>
<h4>11. Working in Other-service field results in having **69.29% lower odds (compared to Adm-clerical)** for getting the above 50k income label</h4>
<h4>12. Working in Prof-specialty results in having **154.84% higher odds (compared to Adm-clerical)** for getting the above 50k income label</h4>
<h4>13. Working in Protective-serv field results in having **31.95% higher odds (compared to Adm-clerical)** for getting the above 50k income label</h4>
<h4>14. Working in sales field results in having **22.62% higher odds (compared to Adm-clerical)** for getting the above 50k income label</h4>
<h4>15. Working in tech-support field results in having **109.9 % higher odds (compared to Adm-clerical)** for getting the above 50k income label</h4>
<h4>16. Working in transport-moving field results in having **46.7% higher odds (compared to Adm-clerical)** for getting the above 50k income label</h4>
<h4>17. Being a male results in having **35.59% higher odds (compared to females)** for getting the above 50k income label</h4>



```{r}
prediction_third <- predict(object = third_model, test_set,type="response")
```

```{r}
head(prediction_third)
```


```{r}
prediction_third_group <- ifelse(prediction_third>=0.5,">50K","<=50K")
```

```{r}
head(prediction_third_group)
```
### Confusion matrix for the third model 

### This time we get 81.92% accuracy score which makes it almost identical with the first model in terms of accuracy, but this time we have 3 less variables so the model is easier to interprete.



```{r}
confusionMatrix(as.factor(prediction_third_group),test_set$income,positive = ">50K")
```

### ROC plot

```{r}
sim_roc3 <- roc(response = test_set$income,
               predictor = prediction_third,
               levels = c('>50K','<=50K'))
```
```{r}
auc(sim_roc3)
```

```{r}
ggroc(sim_roc3, legacy.axes = TRUE) +
  annotate('text', x = .5, y = .5, label = paste0('AUC: ',round(auc(sim_roc3), digits = 2))) +
  theme_minimal()
```


## Models comparison
```{r}
data.frame("sensitivity" = c(sensitivity(as.factor(prediction_group),test_set$income),
                           sensitivity(as.factor(prediction_second_group),test_set$income),
                           sensitivity(as.factor(prediction_third_group),test_set$income)),
           "model" = c("first_model","second_model","third_model"),
           "specificity" = c(specificity(as.factor(prediction_group),test_set$income),
                             specificity(as.factor(prediction_second_group),test_set$income),
                             specificity(as.factor(prediction_third_group),test_set$income)),
           "number_vars" = c(23,19,17),
           "AUC" = c(auc(sim_roc1),auc(sim_roc2),auc(sim_roc3))) %>%
  select(2,1,3,5,4) %>%
  kable(align = "c") %>%
  kable_styling(position="center") 
```

## Conclusion

### It would be suggested to choose the third model as it's simpler (has fewer variables) and its sensitivity, specificity and AUC score is almost identical in comparison with the first and the second model




