---
title: "XGBoost Classification - Poisonous Mushrooms"
author: "Piotr Wieczorek"
date: "2023-10-03"
output: 
  html_document: 
    toc: yes
    highlight: tango
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE)
```

```{r}
rm(list=ls())
```

## Loading packages

```{r}
library(caret) 
library(xgboost) 
library(mltools)
library(data.table)
library(tidyverse)
library(glue)
```

## Loading and exploring the data

```{r}
features <- read.csv('mushroom_features.csv')
targets <- read.csv('mushroom_targets.csv')
```

### Inner joining the data


```{r}
df <- inner_join(features,targets,by="X")
```

```{r}
head(df)
```
### Looking for NA's
```{r}
sum(sapply(df, function(x) sum(is.na(x)))) # No NA's in the dataset
```
```{r}
df <- df %>%
  select(-c(1))
```

### Summary 

```{r}
summary(df)
```

### Exploring the dependent variable - whether or not a particular mushroom is poisonous

```{r}
df %>%
  mutate_if(is.character,as.factor) %>%
  str()
```

### Looking for near zero variance variables

```{r}
nearZeroVar(df)
```

### Near zero variance variables are as follows:
* gill.attachment
* veil.type
* veil.color

#### Technically it's usually possible to modify near zero variance variables to make them more balanced by aggregating some categories, but in this case it wouldn't be helpful so I decided to exclude these variables.

```{r}
print(colnames(df)[6])
print(table(df[,6]))
cat('\n')
print(colnames(df)[16])
print(table(df[,16]))
cat('\n')
print(colnames(df)[17])
print(table(df[,17]))
```

```{r}
df <- df %>%
  select(-c('gill.attachment','veil.type','veil.color'))
```

```{r}
nearZeroVar(df)
```
```{r}
head(df)
```
### Data partition - train and test set

```{r}
data_partition <- createDataPartition(y=df$poisonous, p = 0.7, list = FALSE)

train_set <- df[data_partition,]
test_set <- df[-data_partition,]

print(glue('train_set nrow: {nrow(train_set)}'))
print(glue('train_set ncol: {ncol(train_set)}'))
print(glue('test_set nrow: {nrow(test_set)}'))
print(glue('test_set ncol: {ncol(test_set)}'))
```
```{r}
head(train_set)
```

### Creating dummy variables for both train and test set

```{r}
lab_train <- train_set[,20]
dummy_train <- dummyVars("~ .", data = train_set[,-20])
new_data_train <- data.frame(predict(dummy_train,newdata = train_set[,-20]))
```

```{r}
train_set <- cbind(new_data_train,lab_train)
```



```{r}
lab_test <- test_set[,20]
dummy_test <- dummyVars("~ .", data = test_set[,-20])
new_data_test <- data.frame(predict(dummy_test,newdata = test_set[,-20]))
```


```{r}
test_set <- cbind(new_data_test,lab_test)
```

### Renaming the dependent variable in both train and test set as well as converting it to factor

```{r}
train_set$lab_train <- as.factor(train_set$lab_train)
```


```{r}
colnames(train_set)[111]
```
```{r}
colnames(train_set)[111] <- 'poisonous'
```


```{r}
colnames(test_set)[111]
```
```{r}
test_set$lab_test <- as.factor(test_set$lab_test)
colnames(test_set)[111] <- 'poisonous'
```


```{r}
head(train_set)
```
```{r}
head(test_set)
```
### Finding out whether or not the data set is unbalanced

```{r}
# Imbalanced dataset for train?
table(train_set$poisonous) # No. Won't need to resample.
```
## XGBoost algorithm

#### XGBoost contains following tuning parameters:
* nround - meaning the number of trees/iterations
* max_depth - meaning the maximum level of a signular tree
* eta - meaning learning rate (a number that is going to scale the change in model's parameter)
* gamma - a parameter that decides whether or not a decision tree should be tuned (if branching a particular node into leaf nodes or not is going to be decided based on the contribution of this part of a particular decision tree to minimizing the loss function)
* colsample_bytree - a subsample ratio of columns for a singular tree, e.g. ncol = 100 and colsample_bytree = 0.1 means that each tree is would consider 10 variables
* subsample - sampling X% of the training set, e.g. subsample = 0.5 means that each tree would obtain 50% of the train set rows by random

```{r}
# Doing XGBoost for classification purposes.
grid_tune <- expand.grid(
  nrounds = c(500,1000,1500), #number of trees
  max_depth = c(2,4,6,8,10),
  eta = 0.1, #c(0.025,0.05,0.1,0.3), #Learning rate
  gamma = 0.05, # pruning --> Should be tuned. i.e c(0, 0.05, 0.1, 0.5, 0.7, 0.9, 1.0)
  colsample_bytree = 0.4, # c(0.4, 0.6, 0.8, 1.0) subsample ratio of columns for tree
  min_child_weight = 1, # c(1,2,3) # the larger, the more conservative the model
  #is; can be used as a stop
  subsample = 1 # c(0.5, 0.75, 1.0) # used to prevent overfitting by sampling X% training
)
```

#### trainControl specifies:
* the method (cv stand for cross-validation)
* the number = 3 means that the train set will be divided into 3 parts (samples) and the model will be calculated on one of them and tested on the remaining two, continuing this process for each part (sample)
```{r}
train_control <- trainControl(method = "cv",
                              number=3,
                              verboseIter = TRUE,
                              allowParallel = TRUE)
xgb_tune <- train(x = train_set[,-111],
                  y = train_set[,111],
                  trControl = train_control,
                  tuneGrid = grid_tune,
                  method= "xgbTree",
                  verbose = TRUE)
xgb_tune
```
```{r}
xgb_tune$bestTune
```

#### Using the "best" (although accuracy seems to be constant for different tuning parameters) parameters for the model

```{r}
train_control <- trainControl(method = "none",
                              verboseIter = TRUE,
                              allowParallel = TRUE)
final_grid <- expand.grid(nrounds = xgb_tune$bestTune$nrounds,
                           eta = xgb_tune$bestTune$eta,
                           max_depth = xgb_tune$bestTune$max_depth,
                           gamma = xgb_tune$bestTune$gamma,
                           colsample_bytree = xgb_tune$bestTune$colsample_bytree,
                           min_child_weight = xgb_tune$bestTune$min_child_weight,
                           subsample = xgb_tune$bestTune$subsample)
xgb_model <- train(x = train_set[,-111],
                   y = train_set[,111],
                   trControl = train_control,
                   tuneGrid = final_grid,
                   method = "xgbTree",
                   verbose = TRUE)
```
## Prediction on the test set

```{r}
head(predict(xgb_model, test_set))
```
```{r}
# Prediction:
xgb.pred <- predict(xgb_model, test_set)
```

### Confusion matrix

```{r}
# Confusion Matrix
confusionMatrix(as.factor(as.numeric(xgb.pred)),
                as.factor(as.numeric(test_set$poisonous)))
```

