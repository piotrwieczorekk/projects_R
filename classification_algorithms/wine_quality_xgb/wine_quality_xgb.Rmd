---
title: "Untitled"
author: "Piotr Wieczorek"
date: "2023-10-14"
output: 
  html_document: 
    toc: yes
    theme: readable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message = FALSE,warning = FALSE)
```

## Libraries

```{r}
library(tidyverse)
library(caret)
library(naniar)
library(xgboost)
library(glue)
```


## Reading and cleaning the data

```{r}
wine_features <- read.csv('wine_features.csv')
wine_targets <- read.csv('wine_targets.csv')

df <- inner_join(wine_features,wine_targets,by="X")
```

```{r}
df <- df %>%
  select(-c(1))
```

```{r}
head(df)
```
### Checking the target variable's frequency

```{r}
table(df$quality)
```
### Aggregating the least frequent categories of the target variable

```{r}
df <- df %>%
  mutate(quality = as.factor(quality)) %>%
  mutate(quality = fct_collapse(quality,
                                "Other" = c(3,4,8,9)))
```


```{r}
table(df$quality)
```

## Modelling

### Data partition

```{r}
data_partition <- caret::createDataPartition(y=df$quality,times=1,p=0.75,list=FALSE)

train_set <- df[data_partition,]
test_set <- df[-data_partition,]

print(glue("train_set ncol: {ncol(train_set)}"))
print(glue("train_set nrow: {nrow(train_set)}"))
print(glue("test_set ncol: {ncol(test_set)}"))
print(glue("test_set nrow: {nrow(test_set)}"))
```
```{r}
3657/(3676+1216)
```

```{r}
table(train_set$quality)
```
### Upsampling the train data

```{r}
train_set_upsampled <- caret::upSample(x = train_set[,-12], y = train_set$quality)
```

```{r}
head(train_set_upsampled)
```
```{r}
table(train_set_upsampled$Class)
```
### Explanation of how the algorithm works

XGBoost for multiclass classification problems works as follows:
1) At first it initializes some probability for each row and it's usually 1 divided by the number of classes. So if we have e.g. 3 distinct classes, the initial probability could be set to 1/3 for each row for each class. 
2) Then it calculates the gradient for each row and the gradient is the derivative of the loss function with respect to the derivative of the predicted probability. These gradients represent how much the loss function would change if the predicted probabilities were adjusted. They guide the decision trees e.g. to make splits. In case of multiclass in order to evaluate how wrong the algorithm is, it could use Cross-Entropy as the loss function. So it would have gradients for each row in the data set and then it would use these gradient as "pseudo-residuals" and create the first decision tree based on these residuals. The output of the tree would be logits: as many logits as there are classes to predict. So in case of 3 classes, the tree would output 3 logits (1 logit for 1 class) and it would convert these logits to probabilities by using the softmax function.
3) Then it would take each of these probabilities, multiply it by the learning rate and the whole results would be added to the previous probabilities with respect to each class. This way it would get new predicted probabilities. Then it would come to further iterations, calculate gradients, create another decision trees based on these gradients, treating them as "pseudo-residuals", it would again obtain some logits, transform them to probabilities and multiply them by the learning rate and update the previous probabilities and so on...
4) Points 2-4 are repeated in order to minimize the loss function and therefore to make the predictions as accurate as possible


### Specifying hyperparameters

```{r}
# Doing XGBoost for classification purposes.
grid_tune <- expand.grid(
  nrounds = c(1000,1500), #number of trees
  max_depth = c(8,10),
  eta = 0.1, #c(0.025,0.05,0.1,0.3), #Learning rate
  gamma = 0.05, # pruning --> Should be tuned. i.e c(0, 0.05, 0.1, 0.5, 0.7, 0.9, 1.0)
  colsample_bytree = 0.5, # c(0.4, 0.6, 0.8, 1.0) subsample ratio of columns for tree
  min_child_weight = 1, # c(1,2,3) # the larger, the more conservative the model
  #is; can be used as a stop
  subsample = c(0.75,1) # c(0.5, 0.75, 1.0) # used to prevent overfitting by sampling X% training
)
```

```{r}
set.seed(123)
train_control <- trainControl(method = "cv",
                              number=3,
                              verboseIter = TRUE,
                              allowParallel = TRUE)
xgb_tune <- train(x = train_set_upsampled[,-12],
                  y = train_set_upsampled[,12],
                  trControl = train_control,
                  tuneGrid = grid_tune,
                  method= "xgbTree",
                  verbose = TRUE)
```

### Selecting model with the best hyperparameters

```{r}
xgb_tune$bestTune
```

### Using the model with the best hyperparameters

```{r}
train_control <- trainControl(method = "cv",
                              number=3,
                              verboseIter = TRUE,
                              allowParallel = TRUE)
final_grid <- expand.grid(nrounds = xgb_tune$bestTune$nrounds,
                           eta = xgb_tune$bestTune$eta,
                           max_depth = xgb_tune$bestTune$max_depth,
                           gamma = xgb_tune$bestTune$gamma,
                           colsample_bytree = xgb_tune$bestTune$colsample_bytree,
                           min_child_weight = xgb_tune$bestTune$min_child_weight,
                           subsample = xgb_tune$bestTune$subsample)
xgb_model <- train(x = train_set_upsampled[,-12],
                   y = train_set_upsampled[,12],
                   trControl = train_control,
                   tuneGrid = final_grid,
                   method = "xgbTree",
                   verbose = TRUE)
```

```{r}
head(predict(xgb_model, test_set))
```

```{r}
xgb.pred <- predict(xgb_model, test_set)
```

### Confusion matrix

```{r}
# Confusion Matrix
confusionMatrix(as.factor(xgb.pred),
                as.factor(test_set$quality))
```

1. Accuracy Score: Proportion of correctly classified observations
2. No Information Rate: If we were to guess the most frequent class, what accuracy would we get?
3. Kappa: a metric that considers both the true positive rate and the false positive rate, providing a more balanced assessment of the modelâ€™s performance. Kappa ranges from -1 to 1, with 0 indicating no better than random chance, and 1 indicating perfect agreement between predictions and true values. (https://changjunlee.com/blogs/posts/4_confusion_mat_and_roc)
4. Sensitivity: Proportion of correctly classified positive observations
5. Specificity: Proportion of correctly classified negative observations
6. Pos Pred Value (precision): Out of all of the observations that the model classified as positive, how many of them were actually positive?
7. Neg Pred Value: Out of all of the observations that the model classified as negative, how many of them were actually negative?
8. Prevalence: Proportion of actually positive observations in the data set

