---
title: "Linear Regression Model - Student GPA Score"
author: "Piotr Wieczorek"
date: "2024-04-10"
output: 
  html_document: 
    toc: true
    toc_depth: 4
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = F,message = F)
```

```{r}
rm(list=ls())
```


### Libraries

```{r}
library(tidyverse)
library(scales)
library(ggthemr)
library(skimr)
library(olsrr)
library(moments)
library(car)
library(patchwork)
```



### Reading and exploring the data 


```{r}
df <- read.csv("satgpa.csv")
```


### Explaining variables:
1. sat_v Verbal SAT percentile.
2. sat_m Math SAT percentile.
3. sat_sum Total of verbal and math SAT percentiles.
4. hs_gpa High school grade point average.
5. fy_gpa First year (college) grade point average.


```{r}
head(df)
```


##### Setting up theme


```{r}
ggthemr(palette="dust")
```



```{r}
th <- theme(strip.text = element_text(size=12),
        axis.text.x = element_text(size=12),
        axis.text.y=element_text(size=12),
        plot.title = element_text(size=14))
```



### Relationship between student scores and gender

```{r,fig.width=10,fig.height=6}
df %>%
  mutate(sex=as.factor(sex)) %>%
  gather(c(2:6), value = "val",key="key") %>%
  ggplot(aes(x=sex,y=val,fill=sex)) + 
  geom_boxplot(outlier.shape = NA,alpha=0.7) + 
  geom_jitter(shape=21,position = position_jitter(width=0.1),alpha=0.7,color="black") + 
  facet_wrap(~key,scales="free") + 
  labs(title="Student Scores by Gender") + th
```


### Relationship between fy_gpa and other variabless

```{r,fig.width=10,fig.height=6}
df %>%
  gather(c(2:5),value="val",key="key") %>%
  ggplot(aes(x=val,y=fy_gpa)) +
  geom_point(alpha=1,shape=21) + 
  geom_smooth(method="lm",se=F)+
  facet_wrap(~key,scales="free")+
  labs(x="")+
  th
```


```{r}
swatch()

ggthemr("dust")
```

### Exploring fy_gpa distribution

##### The distribution is skewed to the left (mean is lower than median)
##### Kurtosis is below 3, indicating less residuals (shorter tails) than in a theoretical normal distribution. The distribution is also flatter and more spread out around the mean


```{r,fig.width=10,fig.height=6}
p1 <- df %>%
  ggplot(aes(x=fy_gpa)) + 
  geom_histogram(color="black",alpha=0.7) + 
  labs(y="Count",title="Histogram of fy_gpa")+
  th

p2 <- df %>%
  ggplot(aes(x=fy_gpa)) + 
  geom_density(color="black",fill="#db735c",alpha=0.7) +
  labs(title="Density plot of fy_gpa",y="Density")+
  th

p1/p2
```

```{r}
print(paste("Skewness:",round(skewness(df$fy_gpa),4)))
print(paste("Kurtosis:",round(kurtosis(df$fy_gpa),4)))
```

## Modeling the data


### First model

```{r}
mod1<- lm(data=df,formula = fy_gpa ~. -sat_sum)
```

```{r}
summary(mod1)
```

#### Residual vs Fitted Values plot


##### There is no relationship between residuals and fitted values


```{r}
olsrr::ols_plot_resid_fit(mod1)
```


#### Durbin Watson test

##### Based on p-value > 0.05, there is no autocorrelation of first order in the residuals


```{r}
car::durbinWatsonTest(mod1)
```


#### VIF (Variance Inflation Factor)

##### None of the variables exhibit higher values than 5, which means that the independent variables don't explain each other well (they are no collinearity between them)

```{r}
vif(mod1)
```


#### Cook's distance

##### There are some influential data points which will be investigated upon


```{r}
ols_plot_cooksd_chart(mod1)
```



```{r}
round(4/(1000-4-1),3) #4/(N−k−1) <- formula for Cook's Distance Threshold
```



#### Getting influential data points indexes


```{r}
cooks_indexes <- which(cooks.distance(mod1) > 0.004)
```

```{r}
cooks_indexes
```



#### Summary of data containing only influential data points


```{r}
summary(df[cooks_indexes,2:6])
```


#### Sumary of data excluding influential values


```{r}
summary(df[-cooks_indexes,2:6])
```


### Second model (without influential data points)


```{r}
mod2<- lm(data=df[-cooks_indexes,],formula = fy_gpa ~. -sat_sum)
```


##### R^2 is higher than in the first model, indicating that mod2 captures (explains) more variance in the dependent variable

```{r}
summary(mod2)
```

#### Residual vs Fitted Values plot


##### There is no relationship between residuals and fitted values


```{r}
olsrr::ols_plot_resid_fit(mod2)
```


#### VIF (Variance Inflation Factor)


##### None of the variables exhibit higher values than 5, which means that the independent variables don't explain each other well (they are no collinearity between them)

```{r}
vif(mod2)
```


#### Durbin Watson test

##### Based on p-value > 0.05, there is no autocorrelation of first order in the residuals


```{r}
car::durbinWatsonTest(mod2)
```

```{r}

```

